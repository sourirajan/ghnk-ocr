{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwWxWS3Ex6gr",
        "outputId": "481ba9a6-923b-4a30-d2a8-eaca0f8e7098",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6UFbVHd92OA"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsvaoIeBQLBv"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "import torch\n",
        "import editdistance\n",
        "import evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3I0XKBtQeXf"
      },
      "outputs": [],
      "source": [
        "train_file = '/content/drive/MyDrive/ghnk/train_data.zip'\n",
        "test_file = '/content/drive/MyDrive/ghnk/test_data.zip'\n",
        "\n",
        "audit_log = '/content/drive/MyDrive/ghnk/audit.log'\n",
        "epoch_counter = '/content/drive/MyDrive/ghnk/epoch_counter.log'\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/ghnk/model'\n",
        "processor_dir = '/content/drive/MyDrive/ghnk/processor'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I6n9sB5QnvF"
      },
      "outputs": [],
      "source": [
        "def parse_images_json(file):\n",
        "  images_and_json = []\n",
        "\n",
        "  with zipfile.ZipFile(file, 'r') as zip_ref:\n",
        "    for file_info in tqdm(zip_ref.infolist(), desc='Reading files'):\n",
        "      if file_info.filename.lower().endswith('.jpg'):\n",
        "\n",
        "        json_data = None\n",
        "        cv2_img = None\n",
        "\n",
        "        json_file_info = zip_ref.getinfo(file_info.filename.replace('.jpg', '.json'))\n",
        "\n",
        "        with zip_ref.open(json_file_info) as json_file:\n",
        "          json_data = json.load(json_file)\n",
        "\n",
        "        with zip_ref.open(file_info) as image_file:\n",
        "          img = image_file.read()\n",
        "          img_array = np.frombuffer(img, np.uint8)\n",
        "          cv2_img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
        "          cv2_img = cv2.cvtColor(cv2_img, cv2.COLOR_BGR2RGB)\n",
        "          #images_and_json.append({'images':cv2_img, 'json':json_data})\n",
        "\n",
        "        for data_item in json_data:\n",
        "          text = data_item[\"text\"]\n",
        "          polygon = data_item[\"polygon\"]\n",
        "\n",
        "          coords = np.array([[polygon[\"x0\"], polygon[\"y0\"]],\n",
        "                    [polygon[\"x1\"], polygon[\"y1\"]],\n",
        "                    [polygon[\"x2\"], polygon[\"y2\"]],\n",
        "                    [polygon[\"x3\"], polygon[\"y3\"]]], dtype=np.int32)\n",
        "\n",
        "          # Get the min and max coordinates of x and y\n",
        "          min_x = np.min(coords[:, 0])\n",
        "          min_y = np.min(coords[:, 1])\n",
        "          max_x = np.max(coords[:, 0])\n",
        "          max_y = np.max(coords[:, 1])\n",
        "\n",
        "          cropped_img = cv2_img[min_y:max_y, min_x:max_x]\n",
        "          cropped_pil_image = Image.fromarray(cropped_img)\n",
        "\n",
        "          images_and_json.append({\"image\": cropped_pil_image, \"text\": text})\n",
        "\n",
        "  return images_and_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qknd4wnIaRJG"
      },
      "outputs": [],
      "source": [
        "def get_shuffled_indexes(length):\n",
        "  list_int = [i for i in range(length)]\n",
        "  np.random.seed(42)\n",
        "  np.random.shuffle(list_int)\n",
        "  return list_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjmhAtPDIZO5"
      },
      "outputs": [],
      "source": [
        "def get_images_json(file, split=1):\n",
        "  train_images_json_consolidated = parse_images_json(file)\n",
        "  total_count = len(train_images_json_consolidated)\n",
        "  shuffled_indx = get_shuffled_indexes(total_count)\n",
        "\n",
        "  train_images = [train_images_json_consolidated[i] for i in shuffled_indx[0:int(total_count*split)]]\n",
        "  test_images = [train_images_json_consolidated[j] for j in shuffled_indx[int(total_count*split):]]\n",
        "\n",
        "  return train_images, test_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5upLS0lxo-p"
      },
      "outputs": [],
      "source": [
        "def get_model_and_processor(model_location=None, processor_location=None):\n",
        "\n",
        "  if model_location == None:\n",
        "    model_location = 'microsoft/trocr-base-handwritten'\n",
        "  if processor_location == None:\n",
        "    processor_location = 'microsoft/trocr-base-handwritten'\n",
        "\n",
        "  print(f\"Reading model from {model_location} and processor from {processor_location}\")\n",
        "\n",
        "  processor = TrOCRProcessor.from_pretrained(processor_location)\n",
        "  model = VisionEncoderDecoderModel.from_pretrained(model_location)\n",
        "\n",
        "  model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "  model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
        "  model.config.vocab_size = model.config.decoder.vocab_size\n",
        "\n",
        "  # set beam search parameters\n",
        "  #model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
        "  #model.config.max_length = 64\n",
        "  #model.config.early_stopping = True\n",
        "  #model.config.no_repeat_ngram_size = 3\n",
        "  #model.config.length_penalty = 2.0\n",
        "  #model.config.num_beams = 4\n",
        "\n",
        "  return model, processor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJwoZPy3adwa"
      },
      "outputs": [],
      "source": [
        "class GNHKDataset(Dataset):\n",
        "    def __init__(self, images, processor, max_length=128):\n",
        "        self.images = images\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx][\"image\"]\n",
        "        label = self.images[idx][\"text\"]\n",
        "\n",
        "        pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
        "        # For an image with handwritten text 'week', this is the torch.Size([3, 384, 384]) tensor\n",
        "        #  tensor([[[0.5922, 0.5922, 0.5922,  ..., 0.5059, 0.4980, 0.4980],\n",
        "        #          [0.5922, 0.5922, 0.5922,  ..., 0.5059, 0.4980, 0.4980],\n",
        "        #          [0.5922, 0.5922, 0.5922,  ..., 0.5059, 0.4980, 0.4980],\n",
        "        #          ...,\n",
        "        #          [0.5765, 0.5765, 0.5765,  ..., 0.5059, 0.5137, 0.5137],\n",
        "        #          [0.5765, 0.5765, 0.5765,  ..., 0.5059, 0.5137, 0.5137],\n",
        "        #          [0.5765, 0.5765, 0.5765,  ..., 0.5059, 0.5137, 0.5137]],\n",
        "        #\n",
        "        #          [[0.5294, 0.5294, 0.5294,  ..., 0.4118, 0.4039, 0.4039],\n",
        "        #          [0.5294, 0.5294, 0.5294,  ..., 0.4118, 0.4039, 0.4039],\n",
        "        #          [0.5294, 0.5294, 0.5294,  ..., 0.4118, 0.4039, 0.4039],\n",
        "        #          ...,\n",
        "        #          [0.5137, 0.5137, 0.5137,  ..., 0.4118, 0.4196, 0.4196],\n",
        "        #          [0.5137, 0.5137, 0.5137,  ..., 0.4118, 0.4196, 0.4196],\n",
        "        #          [0.5137, 0.5137, 0.5137,  ..., 0.4118, 0.4196, 0.4196]],\n",
        "        #\n",
        "        #          [[0.3490, 0.3490, 0.3490,  ..., 0.3176, 0.3098, 0.3098],\n",
        "        #          [0.3490, 0.3490, 0.3490,  ..., 0.3176, 0.3098, 0.3098],\n",
        "        #          [0.3490, 0.3490, 0.3490,  ..., 0.3176, 0.3098, 0.3098],\n",
        "        #          ...,\n",
        "        #          [0.3647, 0.3647, 0.3647,  ..., 0.3176, 0.3255, 0.3255],\n",
        "        #          [0.3647, 0.3647, 0.3647,  ..., 0.3176, 0.3255, 0.3255],\n",
        "        #          [0.3647, 0.3647, 0.3647,  ..., 0.3176, 0.3255, 0.3255]]])\n",
        "\n",
        "        val = self.processor.tokenizer(label, padding=\"max_length\", max_length=self.max_length)\n",
        "        # This is a sample 'val' for a label 'week'\n",
        "        # {'input_ids': [0, 3583, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        #                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        #                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        #                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        #                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "        #  'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        #                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        #                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        #                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        #                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
        "        labels = val.input_ids\n",
        "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
        "        # The 'input_ids' for the label 'week', after updating the pad_token_ids to -100\n",
        "        #  tensor([   0, 3583,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
        "        #          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
        "        #          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
        "        #          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
        "        #          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
        "        #          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
        "        #          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
        "        #          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
        "        #          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
        "        #          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
        "        #          -100, -100, -100, -100, -100, -100, -100, -100])\n",
        "\n",
        "        return pixel_values, torch.tensor(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSLgRRQ5a9vB"
      },
      "outputs": [],
      "source": [
        "def train_model(model, processor, train_images, batch_size=12, learning_rate=5e-7):\n",
        "\n",
        "    #model, processor = model_and_processor(model, processor, current_epoch - 1)\n",
        "    train_loader = DataLoader(GNHKDataset(train_images, processor), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #use GPU if available\n",
        "    print(f'Training on device {device}')\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_loss = 0.0\n",
        "    for pixel_values, labels in tqdm(train_loader, desc=f\"(Train)\"):\n",
        "        pixel_values = pixel_values.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    total_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Train Loss: {train_loss / len(train_loader)}\")\n",
        "\n",
        "    return model, processor, total_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWvyDJodbLhl"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, processor, images, batch_size=12):\n",
        "    loader = DataLoader(GNHKDataset(images, processor), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    cer_metric = evaluate.load(\"cer\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    true_texts = []\n",
        "    predicted_texts = []\n",
        "    outputs = []\n",
        "    total_cer = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for pixel_values, labels in tqdm(loader, desc='Evaluation'):\n",
        "            print(labels.shape)\n",
        "            pixel_values = pixel_values.to(device)\n",
        "            generated_ids = model.generate(pixel_values,\n",
        "                                            eos_token_id=processor.tokenizer.sep_token_id,\n",
        "                                            max_length=64,\n",
        "                                            early_stopping=True,\n",
        "                                            no_repeat_ngram_size=3,\n",
        "                                            length_penalty=2.0,\n",
        "                                            num_beams=4)\n",
        "\n",
        "            generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            labels[labels == -100] = processor.tokenizer.pad_token_id\n",
        "            true_labels = processor.batch_decode(labels, skip_special_tokens=True)\n",
        "            print(generated_text, true_labels)\n",
        "            cer = cer_metric.compute(predictions=generated_text, references=true_labels)\n",
        "            total_cer += cer\n",
        "            print(total_cer)\n",
        "\n",
        "        calculated_cer = total_cer/len(loader)\n",
        "        print('Calculated cer is ', calculated_cer)\n",
        "        return calculated_cer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRPoSy9Ik48o"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rajPOD9yISu"
      },
      "outputs": [],
      "source": [
        "model, processor = get_model_and_processor(f\"{model_dir}/8\",f\"{processor_dir}/8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAhtrE_rSTJh"
      },
      "outputs": [],
      "source": [
        "train_images = get_images_json(train_file, 0.8)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB-zLAZoXnsP"
      },
      "outputs": [],
      "source": [
        "model, processor, total_train_loss = train_model(model, processor, train_images)\n",
        "model.save_pretrained(f\"{model_dir}/9\")\n",
        "processor.save_pretrained(f\"{processor_dir}/9\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0ujzO3Zk9uB"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dXGVAEiW_aN"
      },
      "outputs": [],
      "source": [
        "test_images = get_images_json(train_file, 0.8)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSkmcw5Ef9d0"
      },
      "outputs": [],
      "source": [
        "model, processor = get_model_and_processor(f\"{model_dir}/7\", f\"{processor_dir}/7\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LrdcrnQ2AtS"
      },
      "outputs": [],
      "source": [
        "calculated_cer = evaluate_model(model, processor, test_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8uGx7g3lEAC"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brC1uQlI1kGa"
      },
      "outputs": [],
      "source": [
        "eval_images = get_images_json(test_file, 1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1NSzGXpl9c6"
      },
      "outputs": [],
      "source": [
        "model, processor = get_model_and_processor(f\"{model_dir}/9\", f\"{processor_dir}/9\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtRqOnXQbkYo"
      },
      "outputs": [],
      "source": [
        "calculated_cer = evaluate_model(model, processor, eval_images)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}